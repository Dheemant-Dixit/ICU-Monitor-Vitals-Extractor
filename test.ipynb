{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "%pip install -q easyocr\n",
    "%pip install -q ocrd-fork-pylsd==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.test_dataset import LoadImages\n",
    "from perspective import Perspective\n",
    "from screen_ocr import ScreenOCR\n",
    "from models.experimental import attempt_load\n",
    "from utils.torch_utils import select_device\n",
    "from utils.general import non_max_suppression, scale_coords, xyxy2xywh\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipeline(object):\n",
    "    def __init__(self, weights, device=''):\n",
    "        # self.detection_model = None    # TODO complete model loading\n",
    "        self.device = select_device(device)\n",
    "        self.detection_model = attempt_load(weights, map_location=self.device)\n",
    "        self.stride = int(self.detection_model.stride.max())\n",
    "        self.classifier_model = None\n",
    "        self.ppt = Perspective()       # class for handling perspective change\n",
    "        self.ocr = ScreenOCR()         # class for handling OCR part\n",
    "        self.detection_model.eval()\n",
    "        self.classifier_model.eval()\n",
    "\n",
    "    def crop_bboxes(self, image, xywh, margin=15):\n",
    "        h_img, w_img = image.shape\n",
    "        x1 = int(max(0, xywh[0]*w_img-margin))\n",
    "        y1 = int(max(0, xywh[1]*h_img-margin))\n",
    "        x2 = int(min(w_img, xywh[0]*w_img + xywh[2]*w_img + margin))\n",
    "        y2 = int(min(h_img, xywh[1]*h_img + xywh[3]*h_img + margin))\n",
    "        cropped_image = image[:, y1:y2, x1:x2]\n",
    "        warped_image, success = self.ppt.shift_perspective(cropped_image)\n",
    "        return warped_image\n",
    "\n",
    "    def clean_img(self, img):\n",
    "        return img\n",
    "\n",
    "    def evaluate(self, test_data, classify=False):\n",
    "        df = pd.DataFrame(columns=['rr', 'hr', 'spo2', 'map', 'sys', 'dia'])\n",
    "        for path, img, im0 in test_data:\n",
    "            img = torch.from_numpy(img).to(self.device)\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            if img.ndimension() == 3:\n",
    "                img = img.unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                pred = self.detection_model(img, augment=True)[0]\n",
    "            pred = non_max_suppression(pred, 0.25, 0.45, agnostic=True)\n",
    "            det = pred[0]\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "            cropped_image = self.crop_bboxes(im0, xywh)\n",
    "            if classify:\n",
    "                with torch.no_grad():\n",
    "                    screen_types = self.classifier_model(cropped_image)\n",
    "            # cleaning and OCR part\n",
    "            cleaned = self.clean_img(img)\n",
    "            vitals_dict = self.ocr.read_vitals(image=cleaned, img_rgb=img)\n",
    "            df = df.append(vitals_dict, ignore_index=True)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLOv7_WEIGHT  = ''\n",
    "\n",
    "def inference(image_path:str):\n",
    "    \"\"\"\n",
    "    Function responsible for inference.\n",
    "    Args: \n",
    "      image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
    "    Returns:\n",
    "      result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    ### put your code here\n",
    "    t0=time.time()\n",
    "    fpl = FullPipeline(YOLOv7_WEIGHT)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dataset = LoadImages(image_path, img_size=640, stride=fpl.stride)\n",
    "\n",
    "    t2 = time.time()\n",
    "    df = fpl.evaluate(dataset)\n",
    "    \n",
    "\n",
    "    return result    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "022b16dda81504ef81b7e3bb798a6e569a46b7a9000fdc854d50f05eaeb2d706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
