{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.test_dataset import ScreenDataset\n",
    "from utils.perspective import Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.detection_model = None    # TODO complete model loading\n",
    "        self.classifier_model = None\n",
    "        self.ppt = Perspective()\n",
    "\n",
    "    def crop_bboxes(self, images, bboxes, margin=15):\n",
    "        warped_images = []\n",
    "        for image, bbox in zip(images, bboxes):\n",
    "            x, y, w, h = bbox\n",
    "            h_img, w_img = image.shape\n",
    "            x1 = int(max(0, x*w_img-margin))\n",
    "            y1 = int(max(0, y*h_img-margin))\n",
    "            x2 = int(x*w_img + w*w_img + margin)\n",
    "            y2 = int(y*h_img + h*h_img + margin)\n",
    "            cropped_image = image[:, y1:y2, x1:x2]\n",
    "            warped_image, success = self.ppt.shift_perspective(cropped_image)\n",
    "            warped_images.append(warped_image)\n",
    "        return torch.stack(warped_images)\n",
    "\n",
    "    def evaluate(dataloader, classify=True)\n",
    "        for batch_images, _ in dataloader:\n",
    "            with torch.no_grad():\n",
    "                bboxes, confidences = self.detector_model(batch_images)\n",
    "            max_confidences_idx = confidences.argmax(dim=1)\n",
    "            selected_bboxes = bboxes[range(bboxes.shape[0]), max_confienced_idx]\n",
    "            # crop and shift perspective\n",
    "            cropped_images = self.crop_bboxes(batch_images, selected_bboxes)\n",
    "            if classify:\n",
    "                with torch.no_grad():\n",
    "                    screen_types = self.classifier_model(cropped_images)\n",
    "            # cleaning and OCR part\n",
    "            for img in cropped_images:\n",
    "            \n",
    "        # OCR part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_path:str): -> dict\n",
    "  '''\n",
    "  Function responsible for inference.\n",
    "  Args: \n",
    "    image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
    "  Returns:\n",
    "    result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
    "  '''\n",
    "  result = {}\n",
    "\n",
    "  ### put your code here\n",
    "  \n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "022b16dda81504ef81b7e3bb798a6e569a46b7a9000fdc854d50f05eaeb2d706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
